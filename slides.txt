=============================================================================
Estimating and measuring peak performance of numerical ocean model algorithms
=============================================================================

:author: Marshall Ward
:description: Estimating and measuring peak performance of numerical ocean
              model algorithms
:organization: NOAA-GFDL
:date: March 1, 2022
:url: https://marshallward.org/os2022.html
:preface:
   Introductory comments, written to slide notes


Describing Performance
======================

Mathematical performance
   Numerical convergence of algorithm?

Operational performance
   Simulation time per runtime

Computational performance
   Operations per second (FLOPs, Watts ...)


.. notes::

   "Performance" is overall a generic term which different meanings.

   Numerical analysts often focus on accuracy and convergence rates of errors,
   with less discussion of the raw computational cost.

   - If the cost of each iteration is enormous, then convergence rate is a
     poor predictor of runtime.

   Model usage is often measured in throughput, such as "SYPD" or simulated
   years per day.

   - This is a perfect metric, but perhaps too reductive.  Comparison across
     models becomes impossible, and leaves very little sense of how well we
     could have done, or ought to do.

   Computational performance tries to decompose the problem into measurable
   units of calculation, with FLOPs being the most well known.

   - It is also flawed (some reasons we'll address) but is nonetheless a
     useful "unit of work" for calculation, and maps well onto the computer's
     own unit of time (clock frequency), so it has the *potential* to make
     predictive statements of run time.

   No single method can alone characterize performance.

   We generally do a good job quantifying the first two, but reporting the
   absolute performance of a model is uncommon.

   I am hoping that this talk will help to start providing some basis for this
   third point, and to help us start making confident estimates of how we're
   doing, and how we ought to do.


Peak Performance
================

.. math::

   W_\text{FLOPS} = W_\text{core} \times N_\text{cores} \times f_\text{cycles}


============   ===== =========== ========
Platform       Year  Theoretical Observed
============   ===== =========== ========
Cascade Lake   2019  2150.4      2145.3
Volta (GPU)    2018  7065.6      7007.3
============   ===== =========== ========

But are these numbers achievable?


How we compute
--------------

Cascade Lake
   24 core × 8 (AVX512) × 2 (FMA) × 2 port × 2.8 GHz

Volta
   80 SM × 32 FP64 core × 2 (FMA) × 1.38 GHz


Cost of Business
----------------

============   ======== ===== ===== ======
Platform       FLOPs    Year  Price TDP(W)
============   ======== ===== ===== ======
Cascade Lake   2145.3   2019  $6432 240
Volta (GPU)    7007.3   2018  $8999 250
============   ======== ===== ===== ======

.. notes::

   Cascade Lake: Q2'19
   Volta V100: Q1'18


Models
------

Cascade Lake
   Intel(R) Xeon(R) Platinum 8274 CPU @ 3.20GHz

Volta
   Nvidia Volta GV100 (1.38GHz)


Performance Bounds
==================

.. image:: img/dgemm.svg

.. notes::

   * x: length of array, or sqrt(dim) of square matrix

   * Nvidia DGEMM and DAXPY from cuBLAS
   * Intel DGEMM results from MKL
   * Intel DAXPY results from Optiflop

   Note: Optiflop DAXPY results agree with cuBLAS.


Performance Bounds
==================

.. image:: img/dgemm_vs_daxpy.svg

.. notes::

   * x: length of array, or sqrt(dim) of square matrix

   * Nvidia DGEMM and DAXPY from cuBLAS
   * Intel DGEMM results from MKL
   * Intel DAXPY results from Optiflop

   Note: Optiflop DAXPY results agree with cuBLAS.


Compute Bound
=============

Matrix Multiplication (``DGEMM``)

.. math::

   \text{C}_{ij} = \sum_{k} A_{ik} B_{kj}

   \frac{\text{FLOPs}}{\text{byte}}
      = \frac{2n^3}{3n^2 \times \texttt{sizeof}(A)}
      = \frac{1}{12} n

Efficiency (work/byte) increases as problem grows


Memory Bound
============

Field update (``DAXPY``)

.. math::

   \phi_{i} = \phi_{i} + \Delta t \mathcal{F}_{i}

   \frac{\text{FLOPs}}{\text{byte}}
      = \frac{2n}{3n \times \texttt{sizeof} (\phi)}
      = \frac{1}{12}

Bandwidth cost rises in proportion to work


Traditional CFD
===============

Most solvers are hyperbolic and elliptic, e.g.:

.. math::

   \frac{\partial \mathbf{u}}{\partial t} =
         - \mathbf{u} \cdot \nabla \mathbf{u} -\nabla p + \mathcal{F}

   \nabla^2 p = -\nabla \left( \mathbf{u} \cdot \nabla \mathbf{u} \right)
         - \nabla \cdot \mathcal{F} 

Pressure (Montgomery potential, PV, ...) solver is a (sparse) matrix operation.


Ocean CFD
=========

Hydrostatic balance eliminates elliptic step:

.. math::

   \frac{\partial \mathbf{u}}{\partial t} =
         - \mathbf{u} \cdot \nabla \mathbf{u} -\nabla p + \mathcal{F}

   p = -g \rho

   \frac{\partial \rho}{\partial t} = ...

Most barotropic solvers are also hyperbolic.



Array Performance
=================

.. image:: img/cascade_lake_flops.svg

.. notes::

   Difference expressions will exhibit


How these are produced
----------------------

.. code:: c

   int main() {
      /* ... */
      start(timer);
      for (r = 0; r < r_max; r++) {
          for (int i = 0; i < n; i++)
              kernel(i, a, b, x, y);

          // An impossible branch to block loop interchange
          if (y[0] < 0.) dummy(a, b, x, y);
      }
      stop(timer);
   }

   void kernel(int i, double a, double b, double *x, double *y) {
       y[i] = a * x[i] + y[i];
   }

Apply artificially high iteration, but let the compiler construct the kernel.


Unaccounted Factors
-------------------

* Pipelining

* Instruction latency

* Instruction decoder

* Variable Clock Frequency

* Depletion of registers


Arithmetic Intensity
====================

.. image:: img/cascade_lake_roofline.svg
   :width: 60%

.. notes::

   Adjacent lines have the same AI


Roofline Modeling
=================

.. math::

   P = \text{min}(P_\text{max}, I \times B_\text{mem})

* :math:`P`: Performance (FLOP/s)

* :math:`P_\text{max}`: Theoretical peak (FLOP/s)

* :math:`I`: Arithmetic Intensity (FLOP/byte)

* :math:`B_\text{mem}`: Memory bandwidth (byte/s) (cache, RAM, ...)

For our codes, often :math:`P = I \times B_\text{mem}`

.. notes::

   Roofline is a useful model for estimating how well we ought to do

   However, it can be hard to apply in practice due to a few issues

   * The "peak" and "memory bandwidth" are deliberately ambiguous and can be
     hard to assign.  (RAM?  L3<->L2? etc?)

   * No real effort to manage competing timescales in the CPU

   ECM is an alternative model

   We want to keep things very simple and just emphasize the role of bandwidth.


Peak Bandwidth
==============

==========  =========== =========   ================
Platform    Speed (GHz) Width (B)   BW (GB/s)
==========  =========== =========   ================
x86 cache   2.8*        128*        8600
x86 RAM     2.93        8 × 6       141
Volta RAM   877         1024        900
==========  =========== =========   ================

* AVX512 clock speed, L1 load+store, etc

.. notes::

   "x86" is the Cascade Lake used in the previous figures

   "cache" refers to the L1 <-> register transfer speed


x86 Cache Bandwidth
-------------------

.. image:: img/skylake_mem_block_diagram.svg

Three 64B + AGU ports, but only 2 AGU are usable

.. notes::

   https://blogs.fau.de/hager/archives/8683


Euler Step
==========

.. math::

   \phi^{n+1}_i = \phi^n_i
      + \Delta t \left( \frac{\partial \phi}{\partial t} \right)_i

.. code:: c

   y[i] = y[i] + a * x[i]

2 FLOPs per 3×8 bytes: :math:`\frac{1}{12}`


Peak DAXPY
==========

=========== ======== ======   ====
Platform    Observed Theory   %
=========== ======== ======   ====
x86 (Cache) 715.1    716.8    99.7
x86 (RAM)   9.36     11.8     79.3
GPU         67.3     74.8     89.9
=========== ======== ======   ====

.. notes::

   x86 reg: (128 B/c * 2.8 GHz) * 24 cores * (1/8 FLOP/B)?
            (128 B/c * 2.8 GHz) * 24 cores * (1/12 FLOP/B)?

   x86 mem: (2.933GHz * 8 B/c) * (6 channels) * (1/12 FLOP/byte)
      = 11.732


   (NOTE: Bandwidth suggests 1/8 due to 128B/s buses, but the "AGU-limit"
    means only 2 of 3 (2L+S) can be used at a time)

    https://blogs.fau.de/hager/archives/8683

   I can't get this 70.9 number anymore..?  Now getting 67.3 or so.


Finite Difference
=================

.. math::

   \phi^{n+1}_i = \phi^n_i + \frac{\Delta t}{\Delta x^2}
      \left(\phi^n_{i+1} - 2 \phi^n_i + \phi^n_{i-1} \right)

.. code:: c

   y[i] = a * x[i] + b * (x[i+1] +  x[i-1])

   x[i] = y[i]

4 FLOPs (amortized) per 4×8 bytes: :math:`\frac{1}{8}` (...?)


How many FLOPS?
---------------

Is it four or five operations?

.. math::

   \phi^{n+1}_i = \phi^n_i + \frac{\Delta t}{\Delta x^2}
      \left(\phi^n_{i+1} - 2 \phi^n_i + \phi^n_{i-1} \right)

.. code::

   y[i] = x[i] + a * (x[i-1] - 2 * x[i] + x[i+1])

.. math::

   \phi^{n+1}_i = \left( 1 - 2 \frac{\Delta t}{\Delta x^2} \right) \phi^n_i 
      + \frac{\Delta t}{\Delta x^2} \left(\phi^n_{i+1} + \phi^n_{i-1} \right)

.. code::

   y[i] = a * x[i] + b * (x[i-1] + x[i+1])

This is a compiler question: Do we follow parentheses?


Diffusion Performance
=====================

=========== ===== ======   ======
Platform    +1    +8       Theory 
=========== ===== ======   ======
x86 (Cache) 499.9 646.9    1075.2
x86 (RAM)   10.5  10.5     17.6
GPU         81.1  81.1     112.3
=========== ===== ======   ======

Prediction is significantly higher for :math:`I = \frac{1}{8}` ?


Write-Allocated Diffusion
=========================

=========== ===== ======   ======
Platform    +1    +8       Theory
=========== ===== ======   ======
x86 (Cache) 499.9 646.9    716.8
x86 (RAM)   10.5  10.5     11.7
GPU         81.1  81.1     112.3
=========== ===== ======   ======

**Write-allocate**: An x86 write (``y[:] =``) includes a read!

Arithmetic intensity is adjusted to :math:`\frac{1}{12}` (6 moves)

.. notes::

   https://blogs.fau.de/hager/archives/8263

   https://www.cs.virginia.edu/stream/ref.html#counting

   Also, Volta seems to have no such issue


Staging Penalty
===============

============================================ =========   =========
Stage                                        Observed    Theory
============================================ =========   =========
:math:`y \leftarrow a x + b (x^{-} + x^{+})` 1057 GF/s   1075 GF/s
:math:`x \leftarrow y`                       6878 GB/s   8601 GB/s
============================================ =========   =========

Copies in general tend to be suboptimal

(Although better than write-allocate)

.. notes::

   One issue is that optimal copy is different from optimal compute.
   I do not know why.

   Copy is also more sensitive to peel loops than compute.  (Again, dunno why)


.. Square Root
   ===========
   
   * without -ffast-math
   * with -ffast-math


Evaluating Model Performance
============================

.. image:: img/benchmark_topo.svg
   :class: float
   :width: 35%

* 192 × 128 grid, 75 level
  - 32 x 32 per MPI rank
  - ~1.8M points / field

* 288 steps (3 day, :math:`\Delta t = 900s`)

* "Benchmark" configuration:

  - Split barotropic

  - Thermodynamic EOS

  - Parameterization suite


.. notes::

   * So how is MOM6 vectorization?

      * This is a compile-time feature, so ought not depend on scaling

   * Using a "benchmark" single-core test

      * Reasonble set of realistic components
      * A non-trivial land/sea distribution

   * Not necessarily the most physically signficant factors.  Just the ones
     which had a significant impact on performance.


MOM6 Performance
=================

.. image:: img/mom6_subrt_flops.svg


MOM6 Roofline
=============

.. image:: img/mom6_roofline.svg
   :width: 80%


MOM6: All subroutines
---------------------

.. image:: img/mom6_roofline_all_1x.svg
   :width: 80%

Largely memory-bound, with notable exceptions


MOM6: 16x Domain
----------------

.. image:: img/mom6_roofline_all_4x.svg
   :width: 80%

Memory-bound shift as domain size increases


Summary
=======
 
* Most ocean model solvers are bandwidth-limited

* Arithmetic intensity (AI) is a predictor of performance

* CPUs benefit from modest arrays, but can tolerate larger ones

* GPUS *strongly favor* large arrays, and implicit (matrix) solvers

* AI can be used to strategize optimization

Maximize FLOPs **within context of the solver**, but never stop
considering better methods
