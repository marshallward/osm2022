=============================================================================
Estimating and measuring peak performance of numerical ocean model algorithms
=============================================================================

:author: Marshall Ward
:description: Estimating and measuring peak performance of numerical ocean
              model algorithms
:organization: NOAA-GFDL
:date: March 1, 2022
:url: https://marshallward.org/osm2022.html
:preface:
   "Performance" is overall a generic term which different meanings.

   Numerical analysts often focus on accuracy and convergence rates of errors,
   with less discussion of the raw computational cost.

   - If the cost of each iteration is enormous, then convergence rate is a
     poor predictor of runtime.

   Model usage is often measured in throughput, such as "SYPD" or simulated
   years per day.

   - This is a perfect metric, but perhaps too reductive.  Comparison across
     models becomes impossible, and leaves very little sense of how well we
     could have done, or ought to do.

   Computational performance tries to decompose the problem into measurable
   units of calculation, with FLOPs being the most well known.

   - It is also flawed (some reasons we'll address) but is nonetheless a
     useful "unit of work" for calculation, and maps well onto the computer's
     own unit of time (clock frequency), so it has the *potential* to make
     predictive statements of run time.

   No single method can alone characterize performance.

   We generally do a good job quantifying the first two, but reporting the
   absolute performance of a model is uncommon.

   I am hoping that this talk will help to start providing some basis for this
   third point, and to help us start making confident estimates of how we're
   doing, and how we ought to do.
   Introductory comments, written to slide notes


Peak Performance
================

.. math::

   W_\text{FLOPS} = W_\text{core} \times N_\text{cores} \times f_\text{cycles}


============   =========== ========
Platform       Theoretical Observed
============   =========== ========
Cascade Lake   2150.4      2145.3
Volta (GPU)    7065.6      7007.3
============   =========== ========

But are these numbers achievable?


How we compute
--------------

Cascade Lake
   24 core × 8 (AVX512) × 2 (FMA) × 2 port × 2.8 GHz

Volta
   80 SM × 32 FP64 core × 2 (FMA) × 1.38 GHz


Cost of Business
----------------

============   ======== ===== ===== ======
Platform       FLOPs    Year  Price TDP(W)
============   ======== ===== ===== ======
Cascade Lake   2145.3   2019  $6432 240
Volta (GPU)    7007.3   2018  $8999 250
============   ======== ===== ===== ======

.. notes::

   Cascade Lake: Q2'19
   Volta V100: Q1'18


Models
------

Cascade Lake
   Intel(R) Xeon(R) Platinum 8274 CPU @ 3.20GHz

Volta
   Nvidia Volta GV100 (1.38GHz)


Matrix Performance
==================

.. image:: img/dgemm.svg

.. notes::

   * x: length of array, or sqrt(dim) of square matrix

   * Nvidia DGEMM and DAXPY from cuBLAS
   * Intel DGEMM results from MKL
   * Intel DAXPY results from Optiflop

   Note: Optiflop DAXPY results agree with cuBLAS.


Array Performance
=================

.. image:: img/dgemm_vs_daxpy.svg

.. notes::

   * x: length of array, or sqrt(dim) of square matrix

   * Nvidia DGEMM and DAXPY from cuBLAS
   * Intel DGEMM results from MKL
   * Intel DAXPY results from Optiflop

   Note: Optiflop DAXPY results agree with cuBLAS.


Compute Bound
=============

Matrix Multiplication (``DGEMM``)

.. math::

   \text{C}_{ij} = \sum_{k} A_{ik} B_{kj}

   \frac{\text{FLOPs}}{\text{byte}}
      = \frac{2n^3}{3n^2 \times \texttt{sizeof}(A)}
      = \frac{1}{12} n

*Arithmetic intensity* (AI) increases as problem grows


Traditional CFD
---------------

Most solvers are hyperbolic and elliptic, e.g.:

.. math::

   \frac{\partial \mathbf{u}}{\partial t} =
         - \mathbf{u} \cdot \nabla \mathbf{u} -\nabla p + \mathcal{F}

   \nabla^2 p = -\nabla \left( \mathbf{u} \cdot \nabla \mathbf{u} \right)
         - \nabla \cdot \mathcal{F}

Pressure (Montgomery potential, PV, ...) solver is a (sparse) matrix operation.


Memory Bound
============

Field update (``DAXPY``)

.. math::

   \phi_{i} = \phi_{i} + \Delta t \mathcal{F}_{i}

   \frac{\text{FLOPs}}{\text{byte}}
      = \frac{2n}{3n \times \texttt{sizeof} (\phi)}
      = \frac{1}{12}

AI is independent of problem size


Ocean CFD
---------

Hydrostatic balance eliminates elliptic step:

.. math::

   \frac{\partial \mathbf{u}}{\partial t} =
         - \mathbf{u} \cdot \nabla \mathbf{u} -\nabla p + \mathcal{F}

   p = -g \rho

   \frac{\partial \rho}{\partial t} = ...

Most barotropic solvers are also hyperbolic.



Performance Bounds
==================

.. image:: img/cascade_lake_flops.svg

.. notes::

   Difference expressions will exhibit


How these are produced
----------------------

.. code:: c

   int main() {
      /* ... */
      start(timer);
      for (r = 0; r < r_max; r++) {
          for (int i = 0; i < n; i++)
              kernel(i, a, b, x, y);

          // An impossible branch to block loop interchange
          if (y[0] < 0.) dummy(a, b, x, y);
      }
      stop(timer);
   }

   void kernel(int i, double a, double b, double *x, double *y) {
       y[i] = a * x[i] + y[i];
   }

Apply artificially high iteration, but let the compiler construct the kernel.


Unaccounted Factors
-------------------

* Pipelining

* Instruction latency

* Instruction decoder

* Variable Clock Frequency

* Depletion of registers


Roofline Modeling
=================

.. image:: img/cascade_lake_roofline.svg
   :width: 50%

:math:`W_\text{FLOPS} = I \times B_\text{mem}`

.. notes::

   Adjacent lines have the same AI


The Roofline Model
------------------

.. math::

   W = \text{min}(W_\text{max}, I \times B_\text{mem})

* :math:`W`: Performance (FLOP/s)

* :math:`W_\text{max}`: Theoretical peak (FLOP/s)

* :math:`I`: Arithmetic Intensity (FLOP/byte)

* :math:`B_\text{mem}`: Memory bandwidth (byte/s) (cache, RAM, ...)

For our codes, often :math:`W = I \times B_\text{mem}`

.. notes::

   Roofline is a useful model for estimating how well we ought to do

   However, it can be hard to apply in practice due to a few issues

   * The "peak" and "memory bandwidth" are deliberately ambiguous and can be
     hard to assign.  (RAM?  L3<->L2? etc?)

   * No real effort to manage competing timescales in the CPU

   ECM is an alternative model

   We want to keep things very simple and just emphasize the role of bandwidth.


Peak Bandwidth
==============

.. image:: img/skylake_mem_block_diagram.svg
   :class: float
   :width: 45%

=========== =========
Platform    BW (GB/s)
=========== =========
x86 (cache) 8600
x86 (RAM)   141
Volta (RAM) 898
=========== =========

.. notes::

   "x86" is the Cascade Lake used in the previous figures

   "cache" refers to the L1 <-> register transfer speed


Computing Bandwidth
-------------------

==========  =========== =========   ================
Platform    Speed (GHz) Width (B)   BW (GB/s)
==========  =========== =========   ================
x86 cache   2.8*        128*        8600
x86 RAM     2.93        8 × 6       141
Volta RAM   877         1024        898
==========  =========== =========   ================

* AVX512 clock speed, L1 load+store, etc

.. notes::

   "x86" is the Cascade Lake used in the previous figures

   "cache" refers to the L1 <-> register transfer speed


x86 Cache Bandwidth
-------------------

.. image:: img/skylake_mem_block_diagram.svg
   :width: 65%

Three 64B + AGU ports, but only 2 AGU are usable

.. notes::

   https://blogs.fau.de/hager/archives/8683


Euler Step
==========

.. math::

   \phi^{n+1}_i = \phi^n_i
      + \Delta t \left( \frac{\partial \phi}{\partial t} \right)_i

.. code:: c

   y[i] = y[i] + a * x[i]

2 FLOPs per 3×8 bytes: :math:`\frac{1}{12}`


Peak Euler
==========

=========== ======== ======   ====
Platform    Observed Theory   %
=========== ======== ======   ====
x86 (Cache) 715.1    716.8    99.7
x86 (RAM)   9.36     11.8     79.3
GPU         67.3     74.8     89.9
=========== ======== ======   ====

RAM efficiencies of 80-90% are common

.. notes::

   x86 reg: (128 B/c * 2.8 GHz) * 24 cores * (1/8 FLOP/B)?
            (128 B/c * 2.8 GHz) * 24 cores * (1/12 FLOP/B)?

   x86 mem: (2.933GHz * 8 B/c) * (6 channels) * (1/12 FLOP/byte)
      = 11.732

   (NOTE: Bandwidth suggests 1/8 due to 128B/s buses, but the "AGU-limit"
    means only 2 of 3 (2L+S) can be used at a time)

    https://blogs.fau.de/hager/archives/8683

   I can't get this 70.9 number anymore..?  Now getting 67.3 or so.


Diffusion Solver
================

.. math::

   \phi^{n+1}_i = \phi^n_i + \frac{\Delta t}{\Delta x^2}
      \left(\phi^n_{i+1} - 2 \phi^n_i + \phi^n_{i-1} \right)

.. code:: c

   y[i] = a * x[i] + b * x[i+1] + b * x[i-1]

   x[i] = y[i]

4 FLOPs (amortized) per 4×8 bytes: :math:`\frac{1}{8}`


How many FLOPS?
---------------

Is it four or five operations?

.. math::

   \phi^{n+1}_i = \phi^n_i + \frac{\Delta t}{\Delta x^2}
      \left(\phi^n_{i+1} - 2 \phi^n_i + \phi^n_{i-1} \right)

.. code::

   y[i] = x[i] + a * (x[i-1] - 2 * x[i] + x[i+1])

.. math::

   \phi^{n+1}_i = \left( 1 - 2 \frac{\Delta t}{\Delta x^2} \right) \phi^n_i
      + \frac{\Delta t}{\Delta x^2} \left(\phi^n_{i+1} + \phi^n_{i-1} \right)

.. code::

   y[i] = a * x[i] + b * (x[i-1] + x[i+1])

This is a compiler question: Do we follow parentheses?


Loop Unrolling
--------------

Number of unrolls changes the solver:

One unroll:

.. code:: c

   y[i] = a * x[i] + b (x[i-1] + x[i+1])

Two unrolls:

.. code:: c

   y[i] = a * x[i] + b * x[i-1] + b * x[i+1]

.. code:: c

   y[i+2] = a * x[i+2] + b * x[i+1] + b * x[i+3]


Diffusion Performance
=====================

=========== ======== ======   ====
Platform    Observed Theory   %
=========== ======== ======   ====
x86 (Cache) 646.9    1075.2   60.1
x86 (RAM)   10.5     17.6     59.7
GPU         81.1     112.3    72.2
=========== ======== ======   ====

Observed is significantly lower for :math:`I = \frac{1}{8}` ?


Aligned update
--------------

=========== ===== ======   ======
Platform    +1    +8       Theory
=========== ===== ======   ======
x86 (Cache) 499.9 646.9    1075.2
x86 (RAM)   10.5  10.5     17.6
GPU         81.1  81.1     112.3
=========== ===== ======   ======

Single-displacement has a performance penalty


Write-Allocate Effect
=====================

=========== ======== ======   ====
Platform    Observed Theory   %
=========== ======== ======   ====
x86 (Cache) 646.9    1075.2   60.1
x86 (RAM)   10.5     11.7     89.7
GPU         81.1     89.9     90.2
=========== ======== ======   ====

* x86 L1⇋RAM may include a read

* GPU L1⇋RAM observes a :math:`\frac{1}{2}`-read

.. notes::

   https://blogs.fau.de/hager/archives/8263

   https://www.cs.virginia.edu/stream/ref.html#counting

   No idea why GPU is a half-grid, but this is a hardware-implementation issue


Staging Penalty
===============

==========================================   =========   ==========  ====
Stage                                        Observed    Theory      %
==========================================   =========   ==========  ====
:math:`y \leftarrow x + \alpha \nabla^2 x`   1085 GF/s   1434* GF/s  75.7
:math:`x \leftarrow y`                       9145 GB/s   9216* GB/s  99.2
S1 + S2                                      708 GF/s    1075 GF/s   60.1
==========================================   =========   ==========  ====

Restarting on a new array incurs a performance penalty

**NOTE**: L1⇋RAM transfer obscures this cost!

.. notes::

   - AVX512 copy seems to happen at a slightly higher clock speed
     (3.0 vs 2.8 for AVX512 arithmetic)

   - The 


Theoretical values
------------------

Why 1434 GFLOP/s?

* A two-loop unroll exhausts registers!  An additional load is required.

Why 9216 GB/s?

* AVX512 copy is clocked at 3.0 GHz, slightly above AVX512 arithmetic at 2.8
  GHz.


.. Square Root
   ===========

   * without -ffast-math
   * with -ffast-math


Evaluating Model Performance
============================

.. image:: img/benchmark_topo.svg
   :class: float
   :width: 35%

* 192 × 128 grid, 75 level
  - 32 x 32 per MPI rank
  - ~1.8M points / field

* 288 steps (3 day, :math:`\Delta t = 900s`)

* "Benchmark" configuration:

  - Split barotropic

  - Thermodynamic EOS

  - Parameterization suite


.. notes::

   * So how is MOM6 vectorization?

      * This is a compile-time feature, so ought not depend on scaling

   * Using a "benchmark" single-core test

      * Reasonable set of realistic components
      * A non-trivial land/sea distribution

   * Not necessarily the most physically signficant factors.  Just the ones
     which had a significant impact on performance.


MOM6 Performance
=================

.. image:: img/mom6_subrt_flops.svg


MOM6 Roofline
=============

.. image:: img/mom6_roofline.svg
   :width: 80%


.. MOM6: 16x Domain
   ----------------
   
   .. image:: img/mom6_roofline_all_4x.svg
      :width: 80%


MOM6: All subroutines
---------------------

.. image:: img/mom6_roofline_all_1x.svg
   :width: 60%


Recent Speedups
===============

=============  =============     =======
Subroutine     Runtime           Speedup
=============  =============     =======
``hor_visc``   1.69s → 1.01s     1.67x
``corAdCalc``  1.11s → 0.64s     1.74x
=============  =============     =======

Highest points correspond to recent optimization efforts

(NOTE: Timings are from a different config)


Summary
=======
 
* Most ocean model solvers are bandwidth-limited

* Arithmetic intensity (AI) is a predictor of performance

* CPUs benefit from modest arrays, but can tolerate larger ones

* GPUs *strongly favor* large arrays, and implicit (matrix) solvers

* AI can be used to strategize optimization
