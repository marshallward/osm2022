=============================================================================
Estimating and measuring peak performance of numerical ocean model algorithms
=============================================================================

:author: Marshall Ward
:description: Estimating and measuring peak performance of numerical ocean
              model algorithms
:organization: NOAA-GFDL
:date: March 1, 2022
:url: https://marshallward.org/os2022.html
:preface:
   Introductory comments, written to slide notes


Describing Performance
======================

Mathematical performance
   Numerical convergence of algorithm?

Operational performance
   Simulation time per runtime

Computational performance
   Operations per second (watt, etc)


Peak Performance
================

============   =========== ======== ===== ===== ======
Platform       Theoretical Observed Year  Price TDP(W)
============   =========== ======== ===== ===== ======
Broadwell      1555.2      1553.0   2016  $2702 145
Cascade Lake   2150.4      2145.3   2019  $6432 240
Volta (GPU)    7065.6      7005?    2018  $8999 250
============   =========== ======== ===== ===== ======

.. notes::

   Theoretical peaks use frequency-adjusted values for maximum core count.

   Peak: (# cores) x (# ops/instr) x (# instr/cycle) x (freq[max_cores])

   B.W.: 36 * (2*4) * 2 * 2.7 = ??
   C.L.: 24 * (2*4) * 2 * 3.3 GHz = 1267.2
       : 24 * (2*8) * 2 * 2.8 GHz = 2534.4
   V100: 2560 * 2 * 1 * 1.380 GHz = 7065.6

   For cascade lake:
   * AVX512 freq is 2.8, regardless of cores (or so it seems)
   * AVX freq is 

   Broadwell: Q1'16
   Cascade Lake: Q2'19
   Volta V100: Q1'18


GPU performance
===============

.. image:: img/dgemm_vs_daxpy.svg

.. notes::

   * x: length of array, or sqrt(dim) of square matrix

   * Nvidia DGEMM and DAXPY from cuBLAS
   * Intel DGEMM results from MKL
   * Intel DAXPY results from Optiflop

   Note: Optiflop DAXPY results agree with cuBLAS.


Performance Bounds
==================

Compute-bound: Matrix Multiplication

.. math::

   \text{C}_{ij} = \sum_{k} A_{ik} B_{kj}

   \text{AI} = \frac{2n^3}{2n^2 \times \texttt{sizeof}(A)} = \frac{n}{8}

Memory-bound: Field update

.. math::

   \phi_{ijk} = \phi_{ijk} + \Delta t \mathcal{F}_{ijk}

   \text{AI} = \frac{2n}{2n \times \texttt{sizeof} (\phi)} = \frac{1}{8}


FLOP performance
================

.. image:: img/gaea_dp_flops.svg

.. notes::

   Difference expressions will exhibit


Peak DAXPY
==========

Method

Optiflop    10.591
MKL         10.040
Likwid      


Roofline
========

.. image:: img/gaea_roof_dp.svg


Unaccounted Factors
===================

* Pipelining

* Instruction latency

* Instruction decoder

* Variable Clock Frequency

* Depletion of registers


Euler Step
==========

.. math::

   \phi^{n+1}_i = \phi^n_i
      + \Delta t \left( \frac{\partial \phi}{\partial t} \right)_i

.. code:: c

   y[i] = y[i] + a * x[i]

2 FLOPs per 16 bytes: :math:`\frac{1}{8}`


Finite Difference
=================

.. math::

   \phi^{n+1}_i = \phi^n_i + \frac{\Delta t}{\Delta x^2}
      \left(\phi^n_{i+1} - 2 \phi^n_i + \phi^n_{i-1} \right)

.. code:: c

   y[i] = y[i] + a * x[i+1] + b * x[i] + a * x[i-1]

   x[i] = y[i]

5 FLOPs (amortized) per 2 doubles:


.. TODO Two implementations:
   1. +1 offset
   2. +8 offset


Square Root
===========

* without -ffast-math
* with -ffast-math


Hierarchy of Performance
========================

* Implement algorithm

* Measure arithmetic intensity (AI)

* Verify vectorization

  * Eliminate unfavorable structures (**do-if-do**)

* Optimize pipeline: 1 Op/cycle

   * Co-locate read/write access

* Diagnose performance as % of theoretical peak (wrt hardware & AI)




Final
=====

The end

.. notes::

   End slide
